{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series: Credit Card Fraud\n",
    "### Tutorial 2: Generating and Evaluating a Feature Hypothesis\n",
    "\n",
    "**Tags: Foundations, Feature hypotheses, Iterating on a model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tutorial 1 we looked at a basic model for credit card fraud, using the kaggle dataset https://www.kaggle.com/dalpozz/creditcardfraud. This was a nice easy step into enterprise ML use cases because it represents a core component of the type of data product that is valuable in almost all enterprise contexts- prioritized lists. This is the typical way to insert data into an operational workflow in a seamless manner.\n",
    "\n",
    "But we obviously shouldn't consider these models to be stationary. More data will come in over time, our users or adversaries will adjust their behavior, and, most importantly, we'll learn more and more about how our business actually works. In the first two cases we might need only periodic refits, as they represent only updates of information to the same foundation. In the last case, however, we take into account that we might have learned a thing or two about what we're doing- how our business operates, what levers it has, or what new data it might be able to generate. In these scenarios, we will want to update our model construct itself, to take advantage of our new understanding. So how do we go about this? The process I will advocate for here I call the \"ML iteration loop\". It powers a process of continuous improvement and the faster you cycle through it the faster your models improve.\n",
    "\n",
    "1. Generate a hypothesis, based on your domain expertise.\n",
    "2. Generate evidence for the hypothesis on your training data.\n",
    "3. If the evidence supports the hypothesis, aggregate the data/build the feature set that can represent the hypothesis in the model.\n",
    "4. Refit the model. If performance boost, then deploy, else evaluate other merits (eg. Is the new model simpler? More stable? Have more explanatory power?) and decide.\n",
    "\n",
    "In this Tutorial, I'd like to run through an illustrative example of this process in action, using Time as our feature of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generate a Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the Tutorial 1 we chose not to include Time in our feature set. The reason for that was that Time is a unique feature:\n",
    "- it is monotonically increasing\n",
    "- effects on other variables can only run forward\n",
    "- noise in other variables tends to build as it increases\n",
    "\n",
    "among others. Because of this, it wouldn't have been appropriate to include Time naively in our models. Rather, we need to have a working hypothesis for how time should enter into the process we are trying to predict (of note, there are many processes that are time invariant and in those cases time should be explicitly excluded).\n",
    "\n",
    "In our current case we are looking to identify fraudulent transactions. Thinking about the generating process here, fraudulent transactions are generated by fraudsters. So what if a given fraudster or group of fraudsters attacked us in bunches? Ie. What if fraudsters tried to generate a bunch of transactions over a short period of time? Thus we have our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our hypothesis is true, we would expect that fraudulent transactions would come in bunches, and that a given transaction might be more likely to be fraudulent if it is close in time to transactions that were flagged as fraud. This gives us a hook into our data, to generate some evidence for or against our hypothesis. So let's go write some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# We're going to use this to time some things\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data. Don't forget to change this variable to your dataset location.\n",
    "credit_dataset_location = '/Users/ianblumenfeld/data/credit_card_fraud/creditcard.csv'\n",
    "data = pd.read_csv(credit_dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As before, we'll make a copy to use\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most expedient way to pressure test our hypothesis is by generating a heuristic that maps our data onto it. In this case we'll do the following:\n",
    "1. Iterating through our training data, for each data point, create a data set of records from the previous 10 time periods- their associated window.\n",
    "2. Sum up the number of fraudulent records in the created data set, and attach it as a feature to our data.\n",
    "3. Calculate the percentage of fraudulent records for those that had at least one fraudulent record in their associated dataset vs. those that did not. \n",
    "4. If the fraud rate in those with fraud in their window is higher than those without, we consider that we have directional evidence for our \"bunchiness\" hypothesis and we'll try to add it our models. Otherwise we do not.\n",
    "\n",
    "Note that the choice of 10 time periods is arbitrary. We could have chosen 5, 50, even 500. However, this is a place where machine learning differs from causal inference or theoretical physics- we can't reach an absolutely correct result. The No Free Lunch Theorem tells us that a heuristic that performs \"good enough\" is the best we can do. Thus if we'd like to improve, we should be searching for the best one based on performance metrics related to our business needs. In practice, finding the first thing that works tends to be much more valuable than tuning said heuristic after it's been found for a significant amount of time. This is because tuning yields diminishing returns, especially relative to finding new hypothesese to test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284807/284807 [14:23<00:00, 329.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the heuristic value for each record.\n",
    "heuristic_values = []\n",
    "# We'll time this loop. It takes a while which is a thing we'll come back to.\n",
    "for i in tqdm.tqdm(df.index):\n",
    "    val = df[(df.Time<df.iloc[i].Time) & (df.Time>=(df.iloc[i].Time-10))].Class.sum()\n",
    "    heuristic_values.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we expected this loop took a while. This obviously represents a problem if we want to make use of this feature for more work at a later date. Or if we decided we'd like to modify it. There are some ways we can speed it up (through vectorization, eg.), but we'll actually come back to this in a subsequent post with a different approach, utilizing a database for building a feature store, something we'll need anyways for our work to be repeatable.\n",
    "\n",
    "Returning to the problem at hand, we can now add the heuristic values as a feature in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['num_fraud_in_associated_window'] = heuristic_values\n",
    "df['had_fraud_in_associated_window'] = np.array(heuristic_values)>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we check the fraud rate for our two classes (had fraud in the associated window vs. did not), we need to split our data into the training, testing, and validation data. If we don't do so, and check the heuristic on the full dataset we have effectively polluted our analysis and model build by allowing information from the validation set into the training process. Much badness tends to ensue in these scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As before, we'll use the last 75K points for validation.\n",
    "df['validation_set'] = df.index>df.shape[0]-75000\n",
    "df_val = df[df.validation_set==True].copy()\n",
    "\n",
    "# The earlier data we'll use for model training.\n",
    "df_test_train = df[df.validation_set==False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll generate the random split.\n",
    "x_train, x_test = train_test_split(df_test_train, test_size=0.33, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "      <th>num_fraud_in_associated_window</th>\n",
       "      <th>had_fraud_in_associated_window</th>\n",
       "      <th>validation_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54413</th>\n",
       "      <td>46440.0</td>\n",
       "      <td>1.043152</td>\n",
       "      <td>-0.014368</td>\n",
       "      <td>0.234844</td>\n",
       "      <td>1.244115</td>\n",
       "      <td>0.019931</td>\n",
       "      <td>0.344975</td>\n",
       "      <td>0.033729</td>\n",
       "      <td>0.133630</td>\n",
       "      <td>0.037275</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295573</td>\n",
       "      <td>0.659468</td>\n",
       "      <td>-0.315173</td>\n",
       "      <td>0.026232</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39704</th>\n",
       "      <td>39930.0</td>\n",
       "      <td>-0.550111</td>\n",
       "      <td>-0.173865</td>\n",
       "      <td>1.647397</td>\n",
       "      <td>-0.753729</td>\n",
       "      <td>-0.157741</td>\n",
       "      <td>-0.997432</td>\n",
       "      <td>0.402301</td>\n",
       "      <td>-0.234459</td>\n",
       "      <td>-1.842091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472098</td>\n",
       "      <td>0.129285</td>\n",
       "      <td>-0.323370</td>\n",
       "      <td>-0.161022</td>\n",
       "      <td>-0.158018</td>\n",
       "      <td>49.49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53013</th>\n",
       "      <td>45773.0</td>\n",
       "      <td>1.126978</td>\n",
       "      <td>0.314222</td>\n",
       "      <td>0.385173</td>\n",
       "      <td>0.998461</td>\n",
       "      <td>-0.045971</td>\n",
       "      <td>-0.262269</td>\n",
       "      <td>0.121079</td>\n",
       "      <td>-0.019052</td>\n",
       "      <td>-0.445162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264050</td>\n",
       "      <td>0.587558</td>\n",
       "      <td>-0.333059</td>\n",
       "      <td>0.031391</td>\n",
       "      <td>0.012551</td>\n",
       "      <td>13.52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82311</th>\n",
       "      <td>59355.0</td>\n",
       "      <td>-0.953601</td>\n",
       "      <td>-0.691997</td>\n",
       "      <td>1.612222</td>\n",
       "      <td>-1.803030</td>\n",
       "      <td>-2.214743</td>\n",
       "      <td>-0.084287</td>\n",
       "      <td>-0.476875</td>\n",
       "      <td>0.173687</td>\n",
       "      <td>-1.808626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351591</td>\n",
       "      <td>-0.835747</td>\n",
       "      <td>-0.357699</td>\n",
       "      <td>-0.211048</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>152.65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21975</th>\n",
       "      <td>31966.0</td>\n",
       "      <td>1.568981</td>\n",
       "      <td>-0.660334</td>\n",
       "      <td>-0.136480</td>\n",
       "      <td>-1.353745</td>\n",
       "      <td>-0.985982</td>\n",
       "      <td>-1.315005</td>\n",
       "      <td>-0.347411</td>\n",
       "      <td>-0.507443</td>\n",
       "      <td>-2.426676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431909</td>\n",
       "      <td>0.734602</td>\n",
       "      <td>-0.087895</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>0.009205</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time        V1        V2        V3        V4        V5        V6  \\\n",
       "54413  46440.0  1.043152 -0.014368  0.234844  1.244115  0.019931  0.344975   \n",
       "39704  39930.0 -0.550111 -0.173865  1.647397 -0.753729 -0.157741 -0.997432   \n",
       "53013  45773.0  1.126978  0.314222  0.385173  0.998461 -0.045971 -0.262269   \n",
       "82311  59355.0 -0.953601 -0.691997  1.612222 -1.803030 -2.214743 -0.084287   \n",
       "21975  31966.0  1.568981 -0.660334 -0.136480 -1.353745 -0.985982 -1.315005   \n",
       "\n",
       "             V7        V8        V9       ...             V24       V25  \\\n",
       "54413  0.033729  0.133630  0.037275       ...       -0.295573  0.659468   \n",
       "39704  0.402301 -0.234459 -1.842091       ...        0.472098  0.129285   \n",
       "53013  0.121079 -0.019052 -0.445162       ...        0.264050  0.587558   \n",
       "82311 -0.476875  0.173687 -1.808626       ...        0.351591 -0.835747   \n",
       "21975 -0.347411 -0.507443 -2.426676       ...        0.431909  0.734602   \n",
       "\n",
       "            V26       V27       V28  Amount  Class  \\\n",
       "54413 -0.315173  0.026232  0.011719   61.00      0   \n",
       "39704 -0.323370 -0.161022 -0.158018   49.49      0   \n",
       "53013 -0.333059  0.031391  0.012551   13.52      0   \n",
       "82311 -0.357699 -0.211048 -0.166016  152.65      0   \n",
       "21975 -0.087895  0.007436  0.009205   10.00      0   \n",
       "\n",
       "       num_fraud_in_associated_window  had_fraud_in_associated_window  \\\n",
       "54413                               0                           False   \n",
       "39704                               0                           False   \n",
       "53013                               0                           False   \n",
       "82311                               0                           False   \n",
       "21975                               0                           False   \n",
       "\n",
       "       validation_set  \n",
       "54413           False  \n",
       "39704           False  \n",
       "53013           False  \n",
       "82311           False  \n",
       "21975           False  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining the training set, we can see our new feature available.\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of fraudulent records with no fraud in associated window: 0.166535\n",
      "Percentage of fraudulent records with fraud in associated window: 1.010101\n"
     ]
    }
   ],
   "source": [
    "# Now we'll generate the relevant summary statistics.\n",
    "probs = x_train[['Class', 'had_fraud_in_associated_window']].groupby('had_fraud_in_associated_window').agg([sum, len])\n",
    "probs = probs['Class']\n",
    "print('Percentage of fraudulent records with no fraud in associated window: %f' % (probs['sum']/probs['len']*100).loc[False])\n",
    "print('Percentage of fraudulent records with fraud in associated window: %f' % (probs['sum']/probs['len']*100).loc[True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that there is a 6x jump in the fraud rate when the record has fraud in the associated window. To be sure we should check the error bars. To do so we'll use the binomial errors.\n",
    "\n",
    "$$95\\% EB = \\small{+/-}1.96*\\sqrt{\\frac{P*(1-P)}{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_false = (probs['sum']/probs['len']).loc[False]\n",
    "p_true = (probs['sum']/probs['len']).loc[True]\n",
    "n_false = probs['len'].loc[False]\n",
    "n_true = probs['len'].loc[True]\n",
    "eb_false = 1.96*np.sqrt((p_false*(1-p_false))/n_false)\n",
    "eb_true = 1.96*np.sqrt((p_true*(1-p_true))/n_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error bars around percentage with no fraud in associated window: (0.144936, 0.188134)\n",
      "Error bars around percentage with fraud in associated window: (0.686272, 1.333930)\n"
     ]
    }
   ],
   "source": [
    "print(\"Error bars around percentage with no fraud in associated window: (%f, %f)\" % ((p_false-eb_false)*100\n",
    "                                                                                  , (p_false+eb_false)*100))\n",
    "print(\"Error bars around percentage with fraud in associated window: (%f, %f)\" % ((p_true-eb_true)*100\n",
    "                                                                                  , (p_true+eb_true)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10bc6fb00>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAExtJREFUeJzt3XuwXeVdxvHvQwISLS1OczpjE0KwBizTi3TOIBXHUksl\nZTqk2lpJb1Zp06mlOlpRsEoV1KpYx+lILcFSsCqXViYTNTU6imXGNpWDUW6dlAgFEqrEtqEqobn9\n/GNvVjdpcvZKOGvvnHO+n5lM1uXde/1eTpKHd73rkqpCkiSAY8ZdgCTp6GEoSJIahoIkqWEoSJIa\nhoIkqWEoSJIahoIkqWEoSJIahoIkqbFw3AUcrsWLF9fy5cvHXYYkzSp33nnnf1fVxLB2sy4Uli9f\nztTU1LjLkKRZJclDbdp5+kiS1DAUJEkNQ0GS1DAUJEkNQ0GS1DAUJEkNQ0GS1DAUJEkNQ0GSZoGf\nuOZz/MQ1n+v8OIaCJKlhKEiSGoaCJKlhKEiSGoaCJKlhKEiSGoaCJKlhKEiSGoaCJKlhKEiSGoaC\nJKlhKEiSGoaCJKlhKEiSGoaCJKlhKEiSGoaCJKlhKEiSGp2FQpLrkjyW5J5D7H9zkruS3J3ks0le\n2lUtkqR2uhwpXA+snGb/g8ArqurFwJXA2g5rkSS1sLCrL66q25Msn2b/ZwdWNwFLu6pFktTO0TKn\ncBHw6XEXIUnzXWcjhbaSvJJeKPzgNG3WAGsAli1bNqLKJGn+GetIIclLgD8BVlXVVw7VrqrWVtVk\nVU1OTEyMrkBJmmfGFgpJlgG3Am+tqi+Oqw5J0jd1dvooyY3AOcDiJNuADwDHAlTVR4HLgecCH0kC\nsLeqJruqR5I0XJdXH60esv8dwDu6Or4k6fAdLVcfSZKOAoaCJKlhKEiSGoaCJKlhKEiSGoaCJKlh\nKEiSGoaCJKlhKEiSGoaCJKlhKEiSGoaCJKlhKEiSGoaCJKlhKEiSGoaCJKlhKEiSGoaCJKlhKEiS\nGoaCJKlhKEiSGoaCJKlhKEiSGoaCJB3l1m3ezuaHd/L5B7/K2b/zj6zbvL2zY3UWCkmuS/JYknsO\nsT9JPpxka5K7krysq1okabZat3k7l916N7v37Qdg+85dXHbr3Z0FQ5cjheuBldPsfw2wov9rDfDH\nHdYiSbPSVRu3sGvPvqdt27VnH1dt3NLJ8ToLhaq6HfjqNE1WAX9aPZuAE5N8V1f1SNJs9OjOXYe1\n/Zka55zCEuCRgfVt/W2SpL7nn7josLY/U7NiojnJmiRTSaZ27Ngx7nIkaWQuOe80Fh274GnbFh27\ngEvOO62T440zFLYDJw2sL+1v+xZVtbaqJqtqcmJiYiTFSdLR4HVnLOGDP/ZijlvQ++d6yYmL+OCP\nvZjXndHNiZWFnXxrO+uBi5PcBHw/8HhVfXmM9UjSUel1Zyzhxn95GICb3/XyTo/VWSgkuRE4B1ic\nZBvwAeBYgKr6KLABOB/YCjwB/FRXtUiS2uksFKpq9ZD9Bbynq+NLkg7frJholiSNhqEgSWq0On2U\n5BjgpcDzgV3APVX1WJeFSZJGb9pQSPIC4JeBc4H7gR3A8cCpSZ4ArgFuqKr9XRcqSeresJHCb9J7\nJtG7+hPDjSTPA94EvBW4oZvyJEmjNG0oTHcFUf/00R/OeEWSpLE5rInmJN+T5M+S/GWSbu+gkCSN\n3LA5heOr6smBTVcCv9Rf/ivg+7oqTJI0esNGCn+V5G0D63uA5cDJwL6DfkKSNGsNC4WVwLOT/G2S\nHwJ+ETgP+FHgzV0XJ0karWETzfuAP0ryCeDXgHcDv1pV/zGK4iRJozVsTuH7gUuA3cBv07tx7beS\nbAeurKqd3ZcoSRqVYfcpXEPvSabPAj5eVWcDFyZ5BXAzvVNJkqQ5Ylgo7KU3sfwd9EYLAFTVZ4DP\ndFeWJGkchoXCm4B30QuEtw1pK0ma5YaFwv1V9b7pGiTJgY/AkCTNTsMuSb0tyXuTLBvcmOS4JD+c\n5AbgJ7srT5I0SsNGCiuBnwZuTHIKsJPeU1IXAH8H/GFVbe62REnSqAy7T+FJ4CPAR5IcCywGdnkp\nqiTNTa3f0VxVe4Avd1iLJGnMfB2nJKlhKEiSGq1DIcnJSc7tLy9KckJ3ZUmSxqFVKCR5J/Apeo+9\nAFgKrOuqKEnSeLQdKbwHOBv4OkBV3Q88b9iHkqxMsiXJ1iSXHmT/siS3Jdmc5K4k5x9O8ZKkmdU2\nFL5RVc2zj5IsBKa9iznJAuBq4DXA6cDqJKcf0OxXgVuq6gzgQnqXv0qSxqRtKHwmya8Ai5K8Gvgk\nvddxTudMYGtVPdAPlJuAVQe0KeDZ/eXnAI+2rEeS1IG2oXApsAO4m94D8jZU1fuHfGYJ8MjA+rb+\ntkG/DrwlyTZgA/DelvVIkjrQNhTeW1XXVtWPV9UbquraJD83A8dfDVxfVUvpvbfhE0m+paYka5JM\nJZnasWPHDBxWknQwbUPhYA+9e/uQz2wHThpYX9rfNugi4BaAqvocvecqLT7wi6pqbVVNVtXkxMRE\ny5IlSYdr2Os4V9N7p8IpSdYP7DoB+OqQ774DWNF/kN52ehPJbzqgzcPAq4Drk7yQXig4FJCkMRn2\n7KPP0nve0WLgQwPb/we4a7oPVtXeJBcDG+k9VfW6qro3yRXAVFWtB94HXJvk5+lNOr/ddzNI0vgM\ne0rqQ8BDwMuP5MuragO9CeTBbZcPLN9H7/4HSdJRoO0dzWcluSPJ/ybZnWRfkq93XZwkabTaTjT/\nEb0rhe4HFgHvoHdjmiRpDmn9QLyq2gosqKp9VfVxem9lkyTNIW1fsvNEkuOAf0vye/Qmn33stiTN\nMW3/YX9rv+3FwP/Ru//g9V0VJUkaj1Yjhf5VSABPAr8BkORsYGtHdUmSxmDYzWsLgDfSe2bR31bV\nPUleC/wKvQnnM7ovUZI0KsNGCh+jd6roX4APJ3kUmAQurSpfsiNJc8ywUJgEXlJV+5McD/wn8IKq\n+kr3pUmSRm3YRPPuqtoPUFVPAg8YCJI0dw0bKXxvkqeecRTgBf31AFVVL+m0OknSSA0LhReOpApJ\n0lGhzQPxJEnzhHclS5IabR9zIUkao5vfdURvMDhsjhQkSY1hdzTfTe+NaAfl1UeSNLcMO3302v7v\n7+n//on+72/uphxJ0ji1uvooyauravA5R5cm+Vfg0i6LkySNVts5hfSfivrUyg8cxmclSbNE26uP\nLgKuS/Icenczfw346c6qkiSNRdv3KdwJvLQfClTV451WJUkai1ahkOTyA9YBqKorOqhJkjQmbU8f\n/d/A8vH0rkr6wsyXI0kap7anjz40uJ7k94GNnVQkSRqbI72C6NuBpcMaJVmZZEuSrUkOevlqkjcm\nuS/JvUn+4gjrkSTNgLZzCoN3Ni8AJoBp5xP673e+Gng1sA24I8n6qrpvoM0K4DLg7Kr6WpLnHX4X\nJEkzpe2cwmsHlvcC/1VVe4d85kxga1U9AJDkJmAVcN9Am3cCV1fV1wCq6rGW9UiSOtDq9FFVPdS/\nu3kXvZHC85MsG/KxJcAjA+vb+tsGnQqcmuSfk2xKsvJgX5RkTZKpJFM7duxoU7Ik6Qi0CoUkFyS5\nH3gQ+AzwJeDTM3D8hcAK4BxgNXBtkhMPbFRVa6tqsqomJyYmZuCwkqSDaTvRfCVwFvDFqjoFeBWw\nachntgMnDawv7W8btA1YX1V7qupB4Iv0QkKSNAZtQ2FPVX0FOCbJMVV1GzA55DN3ACuSnJLkOOBC\nYP0BbdbRGyWQZDG900kPtC1ekjSz2k4070zyLOB24M+TPMbTb2j7FlW1N8nF9O5nWABcV1X3JrkC\nmKqq9f19P5LkPmAfcEk/fCRJY5CqQ75D55uNku+gN8l8DL13KTwH+PNx/AM+OTlZU1NToz6sJM1q\nSe6sqmFneIaPFPr3G/x1Vb0S2A/cMAP1SZKOQkPnFKpqH7D/qSekSpLmrrZzCv8L3J3k7xmYS6iq\nn+2kKknSWLQNhVv7vyRJc9i0oZBkWVU9XFXOI0jSPDBsTmHdUwtJ/rLjWiRJYzYsFDKw/N1dFiJJ\nGr9hoVCHWJYkzUHDJppfmuTr9EYMi/rL9Nerqp7daXWSpJGaNhSqasGoCpEkjd+Rvo5TkjQHGQqS\npIahIElqGAqSpIahIElqGAqSpIahIElqGAqSpIahIElqGAqSpIahIElqGAqSpIahIElqGAqSpEan\noZBkZZItSbYmuXSadq9PUkkmu6xHkjS9zkIhyQLgauA1wOnA6iSnH6TdCcDPAZ/vqhZJUjtdjhTO\nBLZW1QNVtRu4CVh1kHZXAr8LPNlhLZKkFroMhSXAIwPr2/rbGkleBpxUVX/TYR2SpJbGNtGc5Bjg\nD4D3tWi7JslUkqkdO3Z0X5wkzVNdhsJ24KSB9aX9bU85AXgR8E9JvgScBaw/2GRzVa2tqsmqmpyY\nmOiwZEma37oMhTuAFUlOSXIccCGw/qmdVfV4VS2uquVVtRzYBFxQVVMd1iRJmkZnoVBVe4GLgY3A\nF4BbqureJFckuaCr40qSjtzCLr+8qjYAGw7Ydvkh2p7TZS2SpOG8o1mS1DAUJEkNQ0GS1DAUJEkN\nQ0GS1DAUJEkNQ0GS1DAUJEkNQ0GS1DAUJEkNQ0GS1DAUJEkNQ0GS1DAUJEkNQ0GS1DAUJEkNQ0GS\n1DAUJEkNQ0GS1DAUJEkNQ0GS1DAUJEkNQ0GS1DAUJEmNTkMhycokW5JsTXLpQfb/QpL7ktyV5B+S\nnNxlPZKk6XUWCkkWAFcDrwFOB1YnOf2AZpuByap6CfAp4Pe6qkeSNFyXI4Uzga1V9UBV7QZuAlYN\nNqiq26rqif7qJmBph/VIkoboMhSWAI8MrG/rbzuUi4BPd1iPJGmIheMuACDJW4BJ4BWH2L8GWAOw\nbNmyEVYmSfNLlyOF7cBJA+tL+9ueJsm5wPuBC6rqGwf7oqpaW1WTVTU5MTHRSbGSpG5HCncAK5Kc\nQi8MLgTeNNggyRnANcDKqnqsq0LWbd7OL33qLnbv28+SExdxyXmn8bozpjuTJUnzU2cjharaC1wM\nbAS+ANxSVfcmuSLJBf1mVwHPAj6Z5N+SrJ/pOtZt3s5lt97N7n37Adi+cxeX3Xo36zZ/y6BFkua9\nTucUqmoDsOGAbZcPLJ/b5fEBrtq4hV179j1t2649+7hq4xZHC5J0gDl/R/OjO3cd1nZJms/mfCg8\n/8RFh7VdkuazOR8Kl5x3GouOXfC0bYuOXcAl5502pook6eh1VNyn0KWn5g28+kiShpvzoQC9YDAE\nJGm4OX/6SJLUnqEgSWoYCpKkhqEgSWoYCpKkhqEgSWoYCpKkhqEgSWoYCpKkRqpq3DUcliQ7gIeO\n8OOLgf+ewXJmA/s8P9jn+eGZ9Pnkqhr66spZFwrPRJKpqpocdx2jZJ/nB/s8P4yiz54+kiQ1DAVJ\nUmO+hcLacRcwBvZ5frDP80PnfZ5XcwqSpOnNt5GCJGkaczIUkqxMsiXJ1iSXHmT/tyW5ub//80mW\nj77KmdWiz7+Q5L4kdyX5hyQnj6POmTSszwPtXp+kksz6K1Xa9DnJG/s/63uT/MWoa5xpLf5sL0ty\nW5LN/T/f54+jzpmS5LokjyW55xD7k+TD/f8edyV52YwWUFVz6hewAPgP4LuB44B/B04/oM3PAB/t\nL18I3DzuukfQ51cC395ffvd86HO/3QnA7cAmYHLcdY/g57wC2Ax8Z3/9eeOuewR9Xgu8u798OvCl\ncdf9DPv8Q8DLgHsOsf984NNAgLOAz8/k8efiSOFMYGtVPVBVu4GbgFUHtFkF3NBf/hTwqiQZYY0z\nbWifq+q2qnqiv7oJWDriGmdam58zwJXA7wJPjrK4jrTp8zuBq6vqawBV9diIa5xpbfpcwLP7y88B\nHh1hfTOuqm4HvjpNk1XAn1bPJuDEJN81U8efi6GwBHhkYH1bf9tB21TVXuBx4Lkjqa4bbfo86CJ6\n/6cxmw3tc39YfVJV/c0oC+tQm5/zqcCpSf45yaYkK0dWXTfa9PnXgbck2QZsAN47mtLG5nD/vh+W\nhTP1RZodkrwFmAReMe5aupTkGOAPgLePuZRRW0jvFNI59EaDtyd5cVXtHGtV3VoNXF9VH0rycuAT\nSV5UVfvHXdhsNBdHCtuBkwbWl/a3HbRNkoX0hpxfGUl13WjTZ5KcC7wfuKCqvjGi2royrM8nAC8C\n/inJl+ide10/yyeb2/yctwHrq2pPVT0IfJFeSMxWbfp8EXALQFV9Djie3jOC5qpWf9+P1FwMhTuA\nFUlOSXIcvYnk9Qe0WQ/8ZH/5DcA/Vn8GZ5Ya2uckZwDX0AuE2X6eGYb0uaoer6rFVbW8qpbTm0e5\noKqmxlPujGjzZ3sdvVECSRbTO530wCiLnGFt+vww8CqAJC+kFwo7RlrlaK0H3ta/Cuks4PGq+vJM\nffmcO31UVXuTXAxspHflwnVVdW+SK4CpqloPfIzeEHMrvQmdC8dX8TPXss9XAc8CPtmfU3+4qi4Y\nW9HPUMs+zykt+7wR+JEk9wH7gEuqataOglv2+X3AtUl+nt6k89tn8//kJbmRXrAv7s+TfAA4FqCq\nPkpv3uR8YCvwBPBTM3r8WfzfTpI0w+bi6SNJ0hEyFCRJDUNBktQwFCRJDUNBktQwFCRJDUNBktQw\nFCRJjf8H/NRhvlG+w0QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c48e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot this.\n",
    "plt.errorbar([False, True], [p_false*100, p_true*100], \n",
    "             yerr=[eb_false*100, eb_true*100], \n",
    "             fmt='o')\n",
    "plt.ylabel('Fraud Rate (%)')\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edges of the error bars don't touch, so we've now got very strong evidence for our bunching hypothesis and we can add the related features into our models. In particular we'll add:\n",
    "- had_fraud_in_associated_window and\n",
    "- number_fraudulent_records_in_associated_window.\n",
    "\n",
    "Note that this does not mean that either of these features will be accepted, nor that, if they are, that their contribution will be as strong as the analysis we did above would indicate. This is due to the fact that the time based contribution might be heavily correlated with or even entirely represented by the features we already have available. We'll allow the models themselves to sort that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets, stripping out the unneeded columns.\n",
    "cols = [col for col in x_train.columns if col not in ['Class', 'group', 'Time', 'validation_set']]\n",
    "features_train = x_train[cols]\n",
    "labels_train = x_train[['Class']]\n",
    "features_test = x_test[cols]\n",
    "labels_test = x_test[['Class']]\n",
    "features_validate = df_val[cols]\n",
    "labels_validate = df_val[['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(penalty='l1',).fit(features_train, np.ravel(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for the training set: 0.978290\n",
      "AUC for the training set: 0.979729\n",
      "AUC for the validation set: 0.968051\n"
     ]
    }
   ],
   "source": [
    "auc_train = metrics.roc_auc_score(labels_train, lr_model.predict_proba(features_train.values)[:,1])\n",
    "auc_test = metrics.roc_auc_score(labels_test.values, lr_model.predict_proba(features_test.values)[:,1])\n",
    "auc_val = metrics.roc_auc_score(labels_validate, lr_model.predict_proba(features_validate.values)[:,1])\n",
    "print('AUC for the training set: %f' % auc_train)\n",
    "print('AUC for the training set: %f' % auc_test)\n",
    "print('AUC for the validation set: %f' % auc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model performs on par with the old. So how would we decide if we wanted to deploy it? One thing we could do is check the contribution of the new feature. The fact that we are using L1 regularization helps with this, as it will zero out features it doesn't need. Taking a look at the feature strengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('V1', 0.09047007062501142)\",\n",
       " \"('V2', 0.04479933765342518)\",\n",
       " \"('V3', 0.013423779816884272)\",\n",
       " \"('V4', 0.6237816402878651)\",\n",
       " \"('V5', 0.07411862770269553)\",\n",
       " \"('V6', -0.11678986205366357)\",\n",
       " \"('V7', -0.1303942585674241)\",\n",
       " \"('V8', -0.15317775237915313)\",\n",
       " \"('V9', -0.2512701867755234)\",\n",
       " \"('V10', -0.7781835673050289)\",\n",
       " \"('V11', -0.0724021298194891)\",\n",
       " \"('V12', 0.09860457619419156)\",\n",
       " \"('V13', -0.08950085501623094)\",\n",
       " \"('V14', -0.5567971456493922)\",\n",
       " \"('V15', -0.005031498943219253)\",\n",
       " \"('V16', -0.02856317759488419)\",\n",
       " \"('V17', -0.005183577820944473)\",\n",
       " \"('V18', 0.01734862090105411)\",\n",
       " \"('V19', 0.05219703539383706)\",\n",
       " \"('V20', -0.4569612142793407)\",\n",
       " \"('V21', 0.41522023017911763)\",\n",
       " \"('V22', 0.6899979560701862)\",\n",
       " \"('V23', -0.21182284676605734)\",\n",
       " \"('V24', -0.14068581616109432)\",\n",
       " \"('V25', 0.0)\",\n",
       " \"('V26', 0.22811908287349245)\",\n",
       " \"('V27', -0.8246232428238774)\",\n",
       " \"('V28', -0.2593428462475945)\",\n",
       " \"('Amount', 0.0010179986002064165)\",\n",
       " \"('num_fraud_in_associated_window', 0.0)\",\n",
       " \"('had_fraud_in_associated_window', 0.5598451469010426)\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(tup) for tup in zip(features_train.columns, lr_model.coef_.tolist()[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the \"number of fraudulent records in the associated window\" feature has been dropped by the model, but that the \"had fraud in associated window\" was kept, and has one of the larger contributions to the calculation. Coupling this with the explanatory evidence we generated above would give us justfication to deploy this version vs. the previous one (we may consider ensembling as well, but we'll leave that for a later date).\n",
    "\n",
    "Let's take a look at the deep model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upweight the fraudulent data points.\n",
    "class_weight = {0:1, 1:499}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The model.\n",
    "\n",
    "num_units=50\n",
    "\n",
    "reg = l1_l2()\n",
    "\n",
    "deep_model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "deep_model.add(Dense(num_units, input_dim=features_train.shape[1], \n",
    "                kernel_regularizer=reg, \n",
    "                activation='relu'))\n",
    "deep_model.add(Dropout(0.2))\n",
    "\n",
    "# Layer 2\n",
    "deep_model.add(Dense(num_units, \n",
    "                kernel_regularizer=reg, \n",
    "                activation='relu'))\n",
    "deep_model.add(Dropout(0.2))\n",
    "\n",
    "# Layer 3\n",
    "deep_model.add(Dense(num_units, \n",
    "                kernel_regularizer=reg, \n",
    "                activation='relu'))\n",
    "deep_model.add(Dropout(0.2))\n",
    "\n",
    "# Output\n",
    "deep_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Still using adagrad and binary cross entropy\n",
    "deep_model.compile(optimizer='adagrad', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140571 samples, validate on 69237 samples\n",
      "Epoch 1/100\n",
      "140571/140571 [==============================] - 5s - loss: 8.3273 - acc: 0.7737 - val_loss: 4.4097 - val_acc: 0.9895\n",
      "Epoch 2/100\n",
      "140571/140571 [==============================] - 3s - loss: 6.0967 - acc: 0.8028 - val_loss: 3.8877 - val_acc: 0.9846\n",
      "Epoch 3/100\n",
      "140571/140571 [==============================] - 3s - loss: 5.2168 - acc: 0.8479 - val_loss: 3.7722 - val_acc: 0.9649\n",
      "Epoch 4/100\n",
      "140571/140571 [==============================] - 3s - loss: 4.8120 - acc: 0.8800 - val_loss: 3.4512 - val_acc: 0.9842\n",
      "Epoch 5/100\n",
      "140571/140571 [==============================] - 3s - loss: 4.7376 - acc: 0.8711 - val_loss: 3.3557 - val_acc: 0.9814\n",
      "Epoch 6/100\n",
      "140571/140571 [==============================] - 3s - loss: 4.3733 - acc: 0.9012 - val_loss: 3.2001 - val_acc: 0.9880\n",
      "Epoch 7/100\n",
      "140571/140571 [==============================] - 4s - loss: 4.3808 - acc: 0.8987 - val_loss: 3.5113 - val_acc: 0.8723\n",
      "Epoch 8/100\n",
      "140571/140571 [==============================] - 3s - loss: 4.0901 - acc: 0.8930 - val_loss: 3.1825 - val_acc: 0.9578\n",
      "Epoch 9/100\n",
      "140571/140571 [==============================] - 3s - loss: 4.1927 - acc: 0.8758 - val_loss: 3.1865 - val_acc: 0.9429\n",
      "Epoch 10/100\n",
      "140571/140571 [==============================] - 3s - loss: 3.9626 - acc: 0.8967 - val_loss: 2.9464 - val_acc: 0.9918\n",
      "Epoch 11/100\n",
      "140571/140571 [==============================] - 3s - loss: 3.9769 - acc: 0.9255 - val_loss: 2.9334 - val_acc: 0.9892\n",
      "Epoch 12/100\n",
      "140571/140571 [==============================] - 3s - loss: 3.7712 - acc: 0.9041 - val_loss: 2.8605 - val_acc: 0.9913\n",
      "Epoch 13/100\n",
      "140571/140571 [==============================] - 3s - loss: 3.5704 - acc: 0.9203 - val_loss: 3.1602 - val_acc: 0.8874\n",
      "Epoch 14/100\n",
      "140571/140571 [==============================] - 4s - loss: 3.4132 - acc: 0.9197 - val_loss: 2.7868 - val_acc: 0.9912\n",
      "Epoch 15/100\n",
      "140571/140571 [==============================] - 3s - loss: 3.5339 - acc: 0.9344 - val_loss: 2.9503 - val_acc: 0.9263\n",
      "Epoch 16/100\n",
      "140571/140571 [==============================] - 3s - loss: 3.3620 - acc: 0.9208 - val_loss: 2.7911 - val_acc: 0.9695\n",
      "Epoch 17/100\n",
      "140571/140571 [==============================] - 3s - loss: 3.3904 - acc: 0.9144 - val_loss: 2.9616 - val_acc: 0.9002\n",
      "Epoch 18/100\n",
      "140571/140571 [==============================] - 3s - loss: 3.4254 - acc: 0.9002 - val_loss: 3.0471 - val_acc: 0.8662\n",
      "Epoch 19/100\n",
      "140571/140571 [==============================] - 5s - loss: 3.1922 - acc: 0.9246 - val_loss: 2.7194 - val_acc: 0.9617\n",
      "Epoch 20/100\n",
      "140571/140571 [==============================] - 4s - loss: 3.2471 - acc: 0.9147 - val_loss: 2.7197 - val_acc: 0.9590\n",
      "Epoch 21/100\n",
      "140571/140571 [==============================] - 5s - loss: 3.2081 - acc: 0.9226 - val_loss: 2.6591 - val_acc: 0.9722\n",
      "Epoch 22/100\n",
      "140571/140571 [==============================] - 3s - loss: 3.1791 - acc: 0.9339 - val_loss: 2.6152 - val_acc: 0.9749\n",
      "Epoch 23/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.9953 - acc: 0.9479 - val_loss: 2.5575 - val_acc: 0.9876\n",
      "Epoch 24/100\n",
      "140571/140571 [==============================] - 4s - loss: 3.0491 - acc: 0.9343 - val_loss: 2.5236 - val_acc: 0.9908\n",
      "Epoch 25/100\n",
      "140571/140571 [==============================] - 4s - loss: 3.0092 - acc: 0.9396 - val_loss: 2.5114 - val_acc: 0.9852\n",
      "Epoch 26/100\n",
      "140571/140571 [==============================] - 4s - loss: 3.0287 - acc: 0.9360 - val_loss: 2.5203 - val_acc: 0.9653\n",
      "Epoch 27/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.9706 - acc: 0.9468 - val_loss: 2.4871 - val_acc: 0.9756\n",
      "Epoch 28/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.8771 - acc: 0.9434 - val_loss: 2.4703 - val_acc: 0.9733\n",
      "Epoch 29/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.8171 - acc: 0.9477 - val_loss: 2.3931 - val_acc: 0.9904\n",
      "Epoch 30/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.9859 - acc: 0.9407 - val_loss: 2.4103 - val_acc: 0.9832\n",
      "Epoch 31/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.8718 - acc: 0.9492 - val_loss: 2.4100 - val_acc: 0.9753\n",
      "Epoch 32/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.7895 - acc: 0.9443 - val_loss: 2.4019 - val_acc: 0.9690\n",
      "Epoch 33/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.7199 - acc: 0.9462 - val_loss: 2.3374 - val_acc: 0.9847\n",
      "Epoch 34/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.7248 - acc: 0.9489 - val_loss: 2.3413 - val_acc: 0.9748\n",
      "Epoch 35/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.7308 - acc: 0.9436 - val_loss: 2.3222 - val_acc: 0.9762\n",
      "Epoch 36/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.6723 - acc: 0.9477 - val_loss: 2.2988 - val_acc: 0.9801\n",
      "Epoch 37/100\n",
      "140571/140571 [==============================] - 4s - loss: 2.6305 - acc: 0.9516 - val_loss: 2.2651 - val_acc: 0.9859\n",
      "Epoch 38/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.6881 - acc: 0.9539 - val_loss: 2.2368 - val_acc: 0.9885\n",
      "Epoch 39/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.5696 - acc: 0.9594 - val_loss: 2.2249 - val_acc: 0.9823\n",
      "Epoch 40/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.5943 - acc: 0.9529 - val_loss: 2.2194 - val_acc: 0.9838\n",
      "Epoch 41/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.5068 - acc: 0.9574 - val_loss: 2.1974 - val_acc: 0.9833\n",
      "Epoch 42/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.5678 - acc: 0.9582 - val_loss: 2.2197 - val_acc: 0.9668\n",
      "Epoch 43/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.4954 - acc: 0.9459 - val_loss: 2.1586 - val_acc: 0.9819\n",
      "Epoch 44/100\n",
      "140571/140571 [==============================] - 5s - loss: 2.4822 - acc: 0.9592 - val_loss: 2.1324 - val_acc: 0.9858\n",
      "Epoch 45/100\n",
      "140571/140571 [==============================] - 4s - loss: 2.5468 - acc: 0.9619 - val_loss: 2.1469 - val_acc: 0.9755\n",
      "Epoch 46/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.4213 - acc: 0.9577 - val_loss: 2.1105 - val_acc: 0.9828\n",
      "Epoch 47/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.4107 - acc: 0.9595 - val_loss: 2.0980 - val_acc: 0.9817\n",
      "Epoch 48/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.4260 - acc: 0.9607 - val_loss: 2.1011 - val_acc: 0.9745\n",
      "Epoch 49/100\n",
      "140571/140571 [==============================] - 4s - loss: 2.3730 - acc: 0.9581 - val_loss: 2.0657 - val_acc: 0.9821\n",
      "Epoch 50/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.4342 - acc: 0.9542 - val_loss: 2.0476 - val_acc: 0.9824\n",
      "Epoch 51/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.3776 - acc: 0.9618 - val_loss: 2.0452 - val_acc: 0.9800\n",
      "Epoch 52/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.3324 - acc: 0.9588 - val_loss: 2.0095 - val_acc: 0.9861\n",
      "Epoch 53/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.3325 - acc: 0.9645 - val_loss: 1.9943 - val_acc: 0.9857\n",
      "Epoch 54/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.3472 - acc: 0.9674 - val_loss: 1.9806 - val_acc: 0.9854\n",
      "Epoch 55/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.3337 - acc: 0.9606 - val_loss: 1.9749 - val_acc: 0.9828\n",
      "Epoch 56/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.3189 - acc: 0.9570 - val_loss: 2.0078 - val_acc: 0.9637\n",
      "Epoch 57/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.3272 - acc: 0.9512 - val_loss: 2.0111 - val_acc: 0.9584\n",
      "Epoch 58/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.3277 - acc: 0.9420 - val_loss: 1.9787 - val_acc: 0.9715\n",
      "Epoch 59/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.2494 - acc: 0.9588 - val_loss: 1.9608 - val_acc: 0.9729\n",
      "Epoch 60/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.2234 - acc: 0.9634 - val_loss: 1.9195 - val_acc: 0.9836\n",
      "Epoch 61/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.2514 - acc: 0.9623 - val_loss: 1.9171 - val_acc: 0.9812\n",
      "Epoch 62/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.2348 - acc: 0.9654 - val_loss: 1.9182 - val_acc: 0.9751\n",
      "Epoch 63/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.2179 - acc: 0.9585 - val_loss: 1.9020 - val_acc: 0.9770\n",
      "Epoch 64/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.2110 - acc: 0.9633 - val_loss: 1.8883 - val_acc: 0.9779\n",
      "Epoch 65/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.1809 - acc: 0.9647 - val_loss: 1.8796 - val_acc: 0.9755\n",
      "Epoch 66/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.1625 - acc: 0.9662 - val_loss: 1.8599 - val_acc: 0.9778\n",
      "Epoch 67/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.1496 - acc: 0.9652 - val_loss: 1.8524 - val_acc: 0.9766\n",
      "Epoch 68/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.1384 - acc: 0.9663 - val_loss: 1.8280 - val_acc: 0.9805\n",
      "Epoch 69/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.1216 - acc: 0.9647 - val_loss: 1.8266 - val_acc: 0.9784\n",
      "Epoch 70/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.1203 - acc: 0.9581 - val_loss: 1.8108 - val_acc: 0.9819\n",
      "Epoch 71/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.1131 - acc: 0.9604 - val_loss: 1.7928 - val_acc: 0.9839\n",
      "Epoch 72/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.1035 - acc: 0.9668 - val_loss: 1.8093 - val_acc: 0.9784\n",
      "Epoch 73/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.1073 - acc: 0.9582 - val_loss: 1.7749 - val_acc: 0.9849\n",
      "Epoch 74/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.0952 - acc: 0.9676 - val_loss: 1.7778 - val_acc: 0.9827\n",
      "Epoch 75/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.0517 - acc: 0.9651 - val_loss: 1.7616 - val_acc: 0.9849\n",
      "Epoch 76/100\n",
      "140571/140571 [==============================] - 7s - loss: 2.0653 - acc: 0.9687 - val_loss: 1.7449 - val_acc: 0.9859\n",
      "Epoch 77/100\n",
      "140571/140571 [==============================] - 4s - loss: 2.0072 - acc: 0.9729 - val_loss: 1.7485 - val_acc: 0.9822\n",
      "Epoch 78/100\n",
      "140571/140571 [==============================] - 4s - loss: 2.0457 - acc: 0.9642 - val_loss: 1.7428 - val_acc: 0.9782\n",
      "Epoch 79/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.0316 - acc: 0.9669 - val_loss: 1.7201 - val_acc: 0.9833\n",
      "Epoch 80/100\n",
      "140571/140571 [==============================] - 4s - loss: 2.0405 - acc: 0.9662 - val_loss: 1.7294 - val_acc: 0.9763\n",
      "Epoch 81/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.9796 - acc: 0.9639 - val_loss: 1.6920 - val_acc: 0.9860\n",
      "Epoch 82/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.0095 - acc: 0.9690 - val_loss: 1.6880 - val_acc: 0.9854\n",
      "Epoch 83/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.9571 - acc: 0.9709 - val_loss: 1.6813 - val_acc: 0.9840\n",
      "Epoch 84/100\n",
      "140571/140571 [==============================] - 3s - loss: 2.0229 - acc: 0.9648 - val_loss: 1.6903 - val_acc: 0.9798\n",
      "Epoch 85/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.9667 - acc: 0.9667 - val_loss: 1.6757 - val_acc: 0.9810\n",
      "Epoch 86/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.9256 - acc: 0.9701 - val_loss: 1.6520 - val_acc: 0.9857\n",
      "Epoch 87/100\n",
      "140571/140571 [==============================] - 6s - loss: 1.9932 - acc: 0.9702 - val_loss: 1.6424 - val_acc: 0.9869\n",
      "Epoch 88/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.9418 - acc: 0.9676 - val_loss: 1.6324 - val_acc: 0.9876\n",
      "Epoch 89/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.9486 - acc: 0.9661 - val_loss: 1.6320 - val_acc: 0.9870\n",
      "Epoch 90/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.9135 - acc: 0.9689 - val_loss: 1.6177 - val_acc: 0.9878\n",
      "Epoch 91/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.8956 - acc: 0.9679 - val_loss: 1.6031 - val_acc: 0.9891\n",
      "Epoch 92/100\n",
      "140571/140571 [==============================] - 4s - loss: 1.9001 - acc: 0.9716 - val_loss: 1.5944 - val_acc: 0.9877\n",
      "Epoch 93/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.8677 - acc: 0.9712 - val_loss: 1.5924 - val_acc: 0.98660\n",
      "Epoch 94/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.8818 - acc: 0.9688 - val_loss: 1.5796 - val_acc: 0.9880\n",
      "Epoch 95/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.8753 - acc: 0.9709 - val_loss: 1.5687 - val_acc: 0.9884\n",
      "Epoch 96/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.8745 - acc: 0.9714 - val_loss: 1.5597 - val_acc: 0.9887\n",
      "Epoch 97/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.8726 - acc: 0.9725 - val_loss: 1.5518 - val_acc: 0.9888\n",
      "Epoch 98/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.8643 - acc: 0.9717 - val_loss: 1.5500 - val_acc: 0.9884\n",
      "Epoch 99/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.8413 - acc: 0.9736 - val_loss: 1.5404 - val_acc: 0.9890\n",
      "Epoch 100/100\n",
      "140571/140571 [==============================] - 3s - loss: 1.8037 - acc: 0.9747 - val_loss: 1.5302 - val_acc: 0.9888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b60cc88>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.fit(\n",
    "    features_train.values, \n",
    "    labels_train.values, \n",
    "    batch_size=1024, \n",
    "    epochs=100,\n",
    "    verbose=1, \n",
    "    class_weight=class_weight, \n",
    "    validation_data=(features_test.values, labels_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73440/74999 [============================>.] - ETA: 0s\n",
      "AUC for the training set: 0.977823\n",
      "AUC for the training set: 0.985993\n",
      "AUC for the validation set: 0.976827\n"
     ]
    }
   ],
   "source": [
    "deep_auc_train = metrics.roc_auc_score(labels_train.values, deep_model.predict_proba(features_train.values))\n",
    "deep_auc_test = metrics.roc_auc_score(labels_test.values, deep_model.predict_proba(features_test.values))\n",
    "deep_auc_val = metrics.roc_auc_score(labels_validate.values, deep_model.predict_proba(features_validate.values))\n",
    "print()\n",
    "print('AUC for the training set: %f' % deep_auc_train)\n",
    "print('AUC for the training set: %f' % deep_auc_test)\n",
    "print('AUC for the validation set: %f' % deep_auc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the model performance is effectively the same with this new feature set. We can thus choose whether or not to deploy based on other factors.\n",
    "\n",
    "To finish off, note 3 things that we'll focus on in a subsequent set of posts:\n",
    "1. We constructed a specific heuristic to use for our bunching hypothesis in an ad hoc manner. Are there any algorithmic approaches we could take to optimize this for us? This would improve our model performance while at the same time freeing our cognitive load for further hypothesis generation- a win/win.\n",
    "2. As noted above, the feature generation calculations we ran through were fairly time consuming. This would make it hard to continue operating as we have been, where we load the raw dataset at the start each time. We could (and likely would eventually) spend some time optimizing this, but are there alternatives? In particular we might consider leveraging storage solutions so we can cache some of our calculated results. This offers ancillary benefits as well, such as a way to generate reproducibility.\n",
    "3. In doing the related activities in Tutorials 1 and 2, we needed to copy and past *A LOT* of code. This will tend to make our solutions brittle and error prone, and also difficult to share/collaborate on. How can we address this type of issue?\n",
    "\n",
    "More next time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
